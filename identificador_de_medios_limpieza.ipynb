{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "identificador-de-medios_limpieza.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzSZzlb8GBr17rvvnEF2nk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Murcicrum/Identificador-de-medio/blob/main/identificador_de_medios_limpieza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8qxqfEHoRYV"
      },
      "source": [
        "#Cargado de datos crudos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHpDJa2fsvBZ"
      },
      "source": [
        "import os\n",
        "\n",
        "def get_tweetstext(path_jsons)-> dict: \n",
        "    '''\n",
        "    Dado el path a una carpeta que contenga archivos USUARIOX_tweets.json\n",
        "    Devuelve diccionario con USUARIOX como claves \n",
        "    y una lista con los textos de sus tweets como valor\n",
        "    '''\n",
        "    tweets_text = {}\n",
        "    \n",
        "    for filename in os.listdir( path_jsons ):\n",
        "      texts = []\n",
        "      \n",
        "      if filename[-5:] == '.json':\n",
        "          json_data = json.load( open(path_jsons+filename) )\n",
        "      else: continue\n",
        "      \n",
        "      for element in json_data['data']:\n",
        "          text = element['text']\n",
        "          texts.append(text)\n",
        "          \n",
        "      tweets_text[filename.replace('_tweets.json', '')] = texts\n",
        "        \n",
        "    return tweets_text\n",
        "\n",
        "\n",
        "def print_tweets(tweets_dict, key=None):\n",
        "    '''\n",
        "    Dado un diccionario imprime ordenadamente su contenido\n",
        "    Si una key del mismo es especificada, imprime sólo el valor de esa key\n",
        "    Si no se especifica key se imprimen recursivamente todas \n",
        "    '''\n",
        "    if key:\n",
        "        print('\\n\\n***', key)\n",
        "        for value in tweets_dict[key]:\n",
        "           print('->', value)\n",
        "    else:\n",
        "        for key in tweets_dict:\n",
        "            print_tweets(tweets_dict, key)\n",
        "\n",
        "#Agregar acá el path a una carpeta con .json's\n",
        "path = ''\n",
        "tweets = get_tweetstext(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMARBtZOof0L"
      },
      "source": [
        "##Scrapeo\n",
        "Para los medios Página12 e IPNoticias, cuyos tweets no contienen el titular de la nota sino un fragmento de la misma, extraigo los links a las notas completas y luego hago scraping en ellos para extraer el titular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0TD7UmQvtXA"
      },
      "source": [
        "####Obtener links\n",
        "Cada tweet finaliza con el link a la nota completa, pero en algunos casos este es precedido por otro link a algun sitio adicional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIkxkOOlsypD"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from time import sleep\n",
        "from random import random\n",
        "\n",
        "def get_links(tweets_list) -> list:\n",
        "    links_list = []\n",
        "    for tweet in tweets_list:\n",
        "        i_link = tweet.rfind('http')\n",
        "         \n",
        "        if i_link != -1:                              #i.e. si encontró un link, ya que si el tweet no tiene link rfind devuelve -1\n",
        "            links_list.append( tweet[i_link:] )\n",
        "        \n",
        "    return links_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBy2NadlFN2l"
      },
      "source": [
        "Pero estos links están comprimidos por twitter EJM, no son explicitamente links a los medios EJM. Con lo cual debo hacer un scraper intermedio que visite cada uno de los links comprimidos y extraiga de allí los links completos y reales de los medios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97IvxucQFLi-"
      },
      "source": [
        "def get_reallinks(twitter_links, verbose=False) -> list:\n",
        "    '''\n",
        "    Dada una lista de links comprimidos por twitter\n",
        "    Devuelve los links reales/descomprimidos\n",
        "    '''\n",
        "    real_links = []\n",
        "    \n",
        "    for i, link in enumerate(twitter_links):\n",
        "        if verbose: print('Procesando link ',i, '\\t', link)\n",
        "        try:\n",
        "            response = requests.get(link)\n",
        "            real_links.append(response.url)\n",
        "        except: print('Falló lectura del link', i, '\\t', link)\n",
        "    \n",
        "    return real_links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqjx1ndvpq9"
      },
      "source": [
        "####Extracción de los titulares\n",
        "Defino la funciones que hacen el scraping propiamente dicho. Visitan cada link obtenido con get_link y lo recorren, de cierta forma especifica en cada medio, para extraer el titular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RBQQVvdvoi7"
      },
      "source": [
        "def get_ipn_titles(ipn_links, verbose=False) -> list:\n",
        "    '''\n",
        "    Dada una lista de links de artículos de IPNoticias\n",
        "    Devulve los titulares de dichos artículos\n",
        "    '''\n",
        "    ipn_titles = []\n",
        "\n",
        "    for i,link in enumerate(ipn_links):\n",
        "        if verbose: print('Link ', i, '\\t',link)\n",
        "\n",
        "#Me quedo con los links que remiten a notas escritas, descartando videos y directos\n",
        "        if link[:24]=='https://ip.digital/nota/':                               \n",
        "            response = requests.get(link)\n",
        "            soup = bs(response.content)\n",
        "            try:\n",
        "                title = soup.find_all('title')[0].text\n",
        "                i = title.find(' |')\n",
        "                ipn_titles.append( title[:i] )\n",
        "            except: print('Falló lectura del link ', i)\n",
        "        \n",
        "        elif verbose: print('El link no pertenece a una nota de IPNoticias.\\n')\n",
        "        \n",
        "        sleep(10*random())\n",
        "    return ipn_titles                \n",
        "\n",
        "\n",
        "def get_p12_titles(p12_links, verbose=False) -> list:\n",
        "    '''\n",
        "    Dada una lista de links de artículos de Página12\n",
        "    Devulve los titulares de dichos artículos\n",
        "    '''\n",
        "    p12_titles = []\n",
        "\n",
        "    for i, link in enumerate(p12_links):\n",
        "        \n",
        "        if verbose: print('Link ', i, '\\t',link)\n",
        "        \n",
        "        if link[:28]=='https://www.pagina12.com.ar/':\n",
        "          response = requests.get(link)\n",
        "          soup = bs(response.content)\n",
        "          try:\n",
        "              title = soup.find_all('title')[0].text\n",
        "              i = title.find(' |')\n",
        "              p12_titles.append( title[:i] )\n",
        "          except: print('Falló lectura del link ', i)\n",
        "        \n",
        "        elif verbose : print('El link no pertenece a una nota de Página12.\\n')\n",
        "\n",
        "        sleep(10*random())\n",
        "    \n",
        "    return p12_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-g9WgPfoZQh"
      },
      "source": [
        "#Limpieza\n",
        "Elimino de los tweets todo aquello que no sea el titular de la nota.\n",
        "Dado que cada medio adorna sus tweets de diferentes formas, hay una función para cada medio (excepto Página12 e IPNoticias que salen limpios del scrapeo).\n",
        "Recordemos que en todos los casos de todos los medios, el tweet es finalizado con un link, encontrarlo y eliminarlo es el primer paso en común para todos los medios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTsKsSD_pJK9"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "letters = string.ascii_letters\n",
        "symbols = string.punctuation + '¿¡'\n",
        "\n",
        "def delete_links(tweets_list):\n",
        "    #Elimina el link que esté al final del tweet\n",
        "    #Si el tweet tiene 2 links, hay que pasarlo por esta función 2 veces.\n",
        "    tweets = tweets_list.copy()\n",
        "    for n, tweet in enumerate(tweets):\n",
        "        i_link = tweet.rfind('http')\n",
        "        if i_link != -1:\n",
        "            tweets[n] = tweet[:i_link]\n",
        "            \n",
        "    return tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt6tshstJd6f"
      },
      "source": [
        "Una vez que al contenido de los tweets de todos los medios se le eliminaron los links del final, se pasan los tweets de cada medio por una función particular que los limpia de la forma que corresponda.\n",
        "\n",
        "Por lo general la 'suciedad' en los tweets que queremos eliminar viene en forma de emojis, de hashtags no integrados en el texto, de menciones a las cuentas de los actores de la noticia o a sus redactores,o caracteres especiales utilizados para darle algún formato al tweet. Puede ser que incluso algunos medios publiquen un tweet que no referencie en absoluto a una nota periodística y sea sólo publicidad, una alerta de tránsito o clima, o un retweet a otra cuenta; en dichos casos optamos por descartar el tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgXb4c9qwPu3"
      },
      "source": [
        "####TN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j75dUTa7wSyM"
      },
      "source": [
        "def clean_tn(tn_titles):\n",
        "    titles = delete_links( tn_titles )\n",
        "    \n",
        "    #Algunos titulares bajan 2 lineas y ponen info de le autore\n",
        "    for n, title in enumerate(titles):\n",
        "        i_newline = title.find('\\n\\n')\n",
        "        if i_newline -1:\n",
        "            titles[n] = title[:i_newline]\n",
        "    \n",
        "    return titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXMFGRmTwUnQ"
      },
      "source": [
        "####LN+"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-9ezMWRwYac"
      },
      "source": [
        "def clean_lnmas(lnmas_titles):    \n",
        "    #Algunos titulares tienen doble link\n",
        "    titles = delete_links( lnmas_titles )\n",
        "    titles = delete_links(titles)\n",
        "    \n",
        "    #Los que contienen esos hashtags son publicidad de los programas del canal\n",
        "    titles = [ tit for tit in titles if '#MásMañana' not in tit\n",
        "                                     if  '#MasData'  not in tit \n",
        "                                     if    '#HOY'    not in tit ]\n",
        "    return titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoCxy2rgwcsl"
      },
      "source": [
        "####Clarín"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgMR3c6upLA2"
      },
      "source": [
        "def clean_clarin(clarin_titles):    \n",
        "    titles = delete_links( clarin_titles )\n",
        "    \n",
        "    #Algunos titulares inician con un emoji (que no es letra ni simbolo))\n",
        "    titles = [ title[1:] if title[0] not in (letters+symbols) \n",
        "                         else title\n",
        "                         for title in titles] \n",
        "                         \n",
        "    #Algunos ponen el link del final en una oración que inicia con 'Más:'    \n",
        "    for n,title in enumerate(titles):\n",
        "        i_mas = title.rfind('. Más:')\n",
        "        if i_mas != -1:\n",
        "            titles[n] = title[:i_mas]\n",
        "            \n",
        "    return titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYMkvXNYon8y"
      },
      "source": [
        "#Armado de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUED5D5WWyik"
      },
      "source": [
        "Una vez conseguidos los titulares de las noticias limpios procedimos a preprocesarlos con un lematizador. Buscamos cuales eran los lematizadores disponibles en idioma español y luego de hacer algunas pruebas con cada uno optamos por utilizar NLTK.\n",
        "\n",
        "Una vez tenidos entonces todos los titulares lematizados y sin stopwords, el armado del dataset consistió en guardar cada titular junto con el medio del que provino en un archivo .csv."
      ]
    }
  ]
}